<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
    <meta http-equiv="Pragma" content="no-cache" />
    <meta http-equiv="Expires" content="0" />
    <title>Spelling Bee Assistant</title>
    <style>
      * { box-sizing: border-box; }

      body {
        font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
        margin: 0;
        padding: 24px;
        max-width: 900px;
        background: #1a1a2e;
        color: #e0e0e0;
      }

      /* --- Header / Logo --- */
      .logo {
        display: flex;
        align-items: center;
        gap: 12px;
        margin-bottom: 4px;
      }

      .logo-bee {
        font-size: 2.2rem;
        line-height: 1;
      }

      .logo h1 {
        margin: 0;
        font-size: 1.5rem;
        color: #f5c842;
      }

      .subtitle {
        color: #888;
        margin-bottom: 16px;
      }

      /* --- Cards --- */
      .card {
        border: 1px solid #2a2a4a;
        border-radius: 12px;
        padding: 16px;
        margin: 12px 0;
        background: #16213e;
      }

      .card.hidden { display: none; }

      .card h2 {
        margin-top: 0;
        color: #f5c842;
        font-size: 1.1rem;
      }

      .hint { color: #888; }

      .row {
        display: flex;
        gap: 16px;
        flex-wrap: wrap;
      }

      /* --- Buttons --- */
      button {
        padding: 10px 14px;
        border-radius: 10px;
        border: 1px solid #3a3a5a;
        background: #2a2a4a;
        color: #e0e0e0;
        cursor: pointer;
        font-size: 0.95rem;
        transition: background 0.2s;
      }

      button:hover:not(:disabled) {
        background: #3a3a5a;
      }

      button.primary {
        background: #f5c842;
        color: #1a1a2e;
        border-color: #f5c842;
        font-weight: 600;
      }

      button.primary:hover:not(:disabled) {
        background: #e0b530;
      }

      button.danger {
        background: #c30;
        color: #fff;
        border-color: #c30;
      }

      button.danger:hover:not(:disabled) {
        background: #a00;
      }

      button:disabled {
        opacity: 0.4;
        cursor: not-allowed;
      }

      /* --- Upload area --- */
      .drop-zone {
        border: 2px dashed #3a3a5a;
        border-radius: 12px;
        padding: 2rem;
        text-align: center;
        cursor: pointer;
        transition: border-color 0.2s, background 0.2s;
        position: relative;
      }

      .drop-zone:hover,
      .drop-zone.dragover {
        border-color: #f5c842;
        background: #1e2a4a;
      }

      .drop-zone input[type="file"] {
        position: absolute;
        inset: 0;
        opacity: 0;
        cursor: pointer;
      }

      .drop-zone-icon {
        font-size: 2rem;
        margin-bottom: 0.5rem;
      }

      .drop-zone-text {
        color: #888;
        font-size: 0.9rem;
      }

      .upload-status {
        margin-top: 10px;
        font-size: 0.95rem;
        min-height: 1.4rem;
      }

      .upload-status.ok { color: #4caf50; font-weight: 600; }
      .upload-status.err { color: #ef5350; font-weight: 600; }
      .upload-status.loading { color: #888; }

      /* --- Score bar --- */
      .score-bar {
        display: flex;
        align-items: center;
        gap: 20px;
        padding: 10px 16px;
        background: #1a1a2e;
        border: 1px solid #2a2a4a;
        border-radius: 10px;
        margin-bottom: 12px;
        font-size: 0.9rem;
      }

      .score-item {
        display: flex;
        align-items: center;
        gap: 6px;
      }

      .score-label {
        color: #888;
        font-weight: 600;
        text-transform: uppercase;
        font-size: 0.75rem;
        letter-spacing: 0.04em;
      }

      .score-value {
        font-weight: 700;
        font-size: 1.1rem;
      }

      .score-value.correct { color: #4caf50; }
      .score-value.incorrect { color: #ef5350; }
      .score-value.total { color: #f5c842; }
      .score-value.word { color: #90caf9; font-family: ui-monospace, SFMono-Regular, Menlo, monospace; }

      /* --- Session status bar --- */
      .session-status {
        display: flex;
        align-items: center;
        gap: 12px;
        padding: 12px 16px;
        border: 1px solid #2a2a4a;
        border-radius: 12px;
        margin-bottom: 12px;
        background: #1a1a2e;
      }

      .mic-indicator {
        width: 12px;
        height: 12px;
        border-radius: 50%;
        background: #555;
        transition: background 0.3s;
        flex-shrink: 0;
      }

      .mic-indicator.listening {
        background: #4caf50;
        box-shadow: 0 0 8px #4caf5088;
        animation: pulse 1.5s infinite;
      }

      .mic-indicator.bot-speaking {
        background: #f5c842;
        box-shadow: 0 0 8px #f5c84288;
        animation: pulse 1s infinite;
      }

      .mic-indicator.muted {
        background: #ef5350;
      }

      @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
      }

      .status-label {
        font-size: 0.9rem;
        color: #888;
        flex: 1;
      }

      .session-controls {
        display: flex;
        gap: 8px;
      }

      /* --- Transcript --- */
      .transcript {
        border: 1px solid #2a2a4a;
        border-radius: 12px;
        padding: 16px;
        overflow-y: auto;
        min-height: 300px;
        max-height: calc(100vh - 380px);
        background: #0f1529;
      }

      .transcript-entry {
        margin-bottom: 12px;
        animation: fadeIn 0.2s ease-in;
      }

      @keyframes fadeIn {
        from { opacity: 0; transform: translateY(4px); }
        to { opacity: 1; transform: translateY(0); }
      }

      .transcript-label {
        font-size: 0.75rem;
        font-weight: 700;
        text-transform: uppercase;
        letter-spacing: 0.03em;
        margin-bottom: 2px;
      }

      .transcript-label.user { color: #90caf9; }
      .transcript-label.bot { color: #f5c842; }

      .transcript-text {
        padding: 8px 12px;
        border-radius: 10px;
        font-size: 0.95rem;
        line-height: 1.4;
        display: inline-block;
        max-width: 90%;
      }

      .transcript-entry.user .transcript-text {
        background: #1e2a4a;
        color: #e0e0e0;
      }

      .transcript-entry.bot .transcript-text {
        background: #1a2e1a;
        color: #e0e0e0;
      }

      .transcript-text.interim {
        opacity: 0.5;
        font-style: italic;
      }

      .transcript-empty {
        text-align: center;
        color: #555;
        padding: 3rem 1rem;
        font-size: 0.95rem;
      }
    </style>
  </head>
  <body>
    <div class="logo">
      <span class="logo-bee">&#128029;</span>
      <h1>Spelling Bee Assistant</h1>
    </div>
    <p class="subtitle">Practice spelling with a voice-powered AI tutor</p>

    <!-- Step 1: Upload -->
    <div id="stepUpload" class="card">
      <h2>1) Upload word list image</h2>
      <div class="drop-zone" id="dropZone">
        <input type="file" id="imageFile" accept="image/*" />
        <div class="drop-zone-icon">&#128247;</div>
        <div class="drop-zone-text">
          Drop a word list image here or click to browse
        </div>
      </div>
      <div style="text-align:center; margin:10px 0; color:#666;">— or —</div>
      <button class="primary" id="sampleWordsBtn" style="width:100%;background:#6c63ff;">Use 5 Sample Words (quick test)</button>
      <div class="upload-status" id="uploadStatus"></div>
    </div>

    <!-- Step 2: Connect -->
    <div id="stepConnect" class="card hidden">
      <h2>2) Start practice</h2>
      <p class="hint" style="margin-bottom: 12px;">
        This will request microphone access and connect to the spelling tutor.
      </p>
      <button class="primary" id="startBtn">Start Practice</button>
    </div>

    <!-- Step 3: Session -->
    <div id="sessionView" class="card hidden">
      <h2>3) Voice session</h2>

      <div class="score-bar" id="scoreBar">
        <div class="score-item">
          <span class="score-label">Word</span>
          <span class="score-value word" id="scoreWord">-</span>
        </div>
        <div class="score-item">
          <span class="score-label">Correct</span>
          <span class="score-value correct" id="scoreCorrect">0</span>
        </div>
        <div class="score-item">
          <span class="score-label">Incorrect</span>
          <span class="score-value incorrect" id="scoreIncorrect">0</span>
        </div>
        <div class="score-item">
          <span class="score-label">Progress</span>
          <span class="score-value total" id="scoreProgress">-</span>
        </div>
      </div>

      <div class="session-status">
        <div class="mic-indicator" id="micIndicator"></div>
        <span class="status-label" id="statusLabel">Connecting...</span>
        <div class="session-controls">
          <button id="muteBtn" title="Mute microphone">Mute</button>
          <button class="danger" id="endBtn">End Practice</button>
        </div>
      </div>
      <div class="transcript" id="transcript">
        <div class="transcript-empty">Waiting for conversation to start...</div>
      </div>
    </div>

    <script>
    // ===================== Audio Worklet Processor (inline) =====================
    const workletCode = `
class CaptureProcessor extends AudioWorkletProcessor {
  constructor(options) {
    super();
    this.targetRate = 16000;
    this.nativeRate = options.processorOptions.sampleRate || sampleRate;
    this.ratio = this.nativeRate / this.targetRate;
    // 32ms at 16kHz = 512 samples
    this.chunkSize = 512;
    this.buffer = new Float32Array(0);
    this.resamplePos = 0;
    this.muted = false;
    this.port.onmessage = (e) => {
      if (e.data.type === 'mute') this.muted = e.data.value;
    };
  }

  process(inputs) {
    const input = inputs[0];
    if (!input || !input[0]) return true;
    const samples = input[0];

    // Resample to 16kHz using linear interpolation
    const outputLen = Math.floor(samples.length / this.ratio);
    const resampled = new Float32Array(outputLen);
    for (let i = 0; i < outputLen; i++) {
      const srcIdx = i * this.ratio;
      const idx0 = Math.floor(srcIdx);
      const idx1 = Math.min(idx0 + 1, samples.length - 1);
      const frac = srcIdx - idx0;
      resampled[i] = samples[idx0] * (1 - frac) + samples[idx1] * frac;
    }

    // Append to buffer
    const newBuf = new Float32Array(this.buffer.length + resampled.length);
    newBuf.set(this.buffer);
    newBuf.set(resampled, this.buffer.length);
    this.buffer = newBuf;

    // Emit 512-sample chunks
    while (this.buffer.length >= this.chunkSize) {
      const chunk = this.buffer.slice(0, this.chunkSize);
      this.buffer = this.buffer.slice(this.chunkSize);

      // Convert float32 to int16
      const pcm = new Int16Array(this.chunkSize);
      for (let i = 0; i < this.chunkSize; i++) {
        const s = this.muted ? 0 : Math.max(-1, Math.min(1, chunk[i]));
        pcm[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      this.port.postMessage({ type: 'audio', pcm: pcm.buffer }, [pcm.buffer]);
    }
    return true;
  }
}
registerProcessor('capture-processor', CaptureProcessor);
`;

    // ===================== State =====================
    let sessionId = null;
    let totalWords = 0;
    let ws = null;
    let audioCtx = null;
    let micStream = null;
    let workletNode = null;
    let isMuted = false;
    let isBotSpeaking = false;

    // Playback queue
    let playbackCtx = null;
    let nextPlayTime = 0;
    let lastMessageTime = 0;
    const PLAY_TIME_RESET_THRESHOLD = 1.0; // seconds
    let activeSources = [];

    // Transcript
    let currentUserEntry = null;
    let currentBotEntry = null;

    // Score tracking (parsed from bot transcripts)
    let scoreCorrect = 0;
    let scoreIncorrect = 0;
    let currentWord = '-';

    // ===================== DOM refs =====================
    const stepUpload = document.getElementById('stepUpload');
    const stepConnect = document.getElementById('stepConnect');
    const sessionView = document.getElementById('sessionView');
    const dropZone = document.getElementById('dropZone');
    const imageFile = document.getElementById('imageFile');
    const uploadStatus = document.getElementById('uploadStatus');
    const sampleWordsBtn = document.getElementById('sampleWordsBtn');
    const startBtn = document.getElementById('startBtn');
    const muteBtn = document.getElementById('muteBtn');
    const endBtn = document.getElementById('endBtn');
    const micIndicator = document.getElementById('micIndicator');
    const statusLabel = document.getElementById('statusLabel');
    const transcript = document.getElementById('transcript');
    const scoreWordEl = document.getElementById('scoreWord');
    const scoreCorrectEl = document.getElementById('scoreCorrect');
    const scoreIncorrectEl = document.getElementById('scoreIncorrect');
    const scoreProgressEl = document.getElementById('scoreProgress');

    function updateScoreUI() {
      scoreWordEl.textContent = currentWord;
      scoreCorrectEl.textContent = scoreCorrect;
      scoreIncorrectEl.textContent = scoreIncorrect;
      const attempted = scoreCorrect + scoreIncorrect;
      scoreProgressEl.textContent = totalWords > 0 ? `${attempted} / ${totalWords}` : '-';
    }

    // ===================== Score parsing from bot text =====================
    function parseScoreFromText(text) {
      if (!text) return;

      // Detect correct
      if (/\bcorrect\b/i.test(text) && !/\bincorrect\b/i.test(text) && !/\bnot\s+(quite\s+)?correct\b/i.test(text) ||
          /\bthat'?s\s+(right|correct)\b/i.test(text) ||
          /\bwell\s+done\b/i.test(text) ||
          /\bgreat\s+(job|spelling|work)\b/i.test(text) ||
          /\bperfect\b/i.test(text) ||
          /\byou\s+(got|spelled|nailed)\s+it/i.test(text) ||
          /\bnice\s+(job|work|spelling)\b/i.test(text) ||
          /\bexactly\s*(right|!|\.)/i.test(text) ||
          /\bawesome\b/i.test(text) ||
          /\bexcellent\b/i.test(text)) {
        scoreCorrect++;
        updateScoreUI();
      } else if (/\bincorrect\b/i.test(text) ||
                 /\bnot\s+(quite|right|correct)\b/i.test(text) ||
                 /\blet'?s\s+try\s+(again|that)\b/i.test(text) ||
                 /\bthe\s+correct\s+spelling\s+is\b/i.test(text) ||
                 /\bspelled\s+(it\s+)?wrong/i.test(text) ||
                 /\boops\b/i.test(text) ||
                 /\bnot\s+the\s+right\b/i.test(text) ||
                 /\bclose,?\s+but\b/i.test(text) ||
                 /\balmost\b/i.test(text)) {
        scoreIncorrect++;
        updateScoreUI();
      }

      // Detect current word — match many announcement patterns
      const wordPatterns = [
        /(?:first|next|new|your)\s+(?:next\s+)?word\s+is[:\s]+["']?(\w+)["']?/i,
        /\bthe\s+word\s+is[:\s]+["']?(\w+)["']?/i,
        /\bspell(?:\s+the\s+word)?[:\s]+["']?(\w+)["']?/i,
        /\bhere(?:'s|\s+is)\s+(?:your\s+)?(?:next\s+)?word[:\s]+["']?(\w+)["']?/i,
        /\bword\s+(?:number\s+)?\d+[:\s]+["']?(\w+)["']?/i,
        /\btry\s+(?:to\s+)?spell[:\s]+["']?(\w+)["']?/i,
        /\bcan\s+you\s+spell[:\s]+["']?(\w+)["']?/i,
      ];
      for (const pat of wordPatterns) {
        const m = text.match(pat);
        if (m && m[1] && m[1].length > 1) {
          currentWord = m[1].toLowerCase();
          updateScoreUI();
          break;
        }
      }
    }

    // ===================== Upload =====================
    dropZone.addEventListener('dragover', (e) => {
      e.preventDefault();
      dropZone.classList.add('dragover');
    });

    dropZone.addEventListener('dragleave', () => {
      dropZone.classList.remove('dragover');
    });

    dropZone.addEventListener('drop', (e) => {
      e.preventDefault();
      dropZone.classList.remove('dragover');
      if (e.dataTransfer.files.length) {
        imageFile.files = e.dataTransfer.files;
        handleUpload(e.dataTransfer.files[0]);
      }
    });

    imageFile.addEventListener('change', () => {
      if (imageFile.files.length) {
        handleUpload(imageFile.files[0]);
      }
    });

    // ===================== Sample Words (quick test) =====================
    sampleWordsBtn.addEventListener('click', async () => {
      sampleWordsBtn.disabled = true;
      sampleWordsBtn.textContent = 'Loading...';
      uploadStatus.className = 'upload-status loading';
      uploadStatus.textContent = 'Generating sample words...';

      try {
        const res = await fetch('/sample-words', { method: 'POST' });
        const data = await res.json();

        if (!res.ok) {
          uploadStatus.className = 'upload-status err';
          uploadStatus.textContent = `Failed: ${data.detail || res.statusText}`;
          return;
        }

        sessionId = data.session_id;
        totalWords = data.word_count || 0;

        uploadStatus.className = 'upload-status ok';
        uploadStatus.textContent = `${totalWords} sample words: ${data.sample.join(', ')}`;

        stepConnect.classList.remove('hidden');
        stepConnect.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
      } catch (err) {
        uploadStatus.className = 'upload-status err';
        uploadStatus.textContent = `Failed: ${err.message}`;
      } finally {
        sampleWordsBtn.disabled = false;
        sampleWordsBtn.textContent = 'Use 5 Sample Words (quick test)';
      }
    });

    let uploadTimer = null;

    function startUploadTimer() {
      let seconds = 0;
      uploadStatus.className = 'upload-status loading';
      uploadStatus.textContent = 'Processing... 0s';
      uploadTimer = setInterval(() => {
        seconds++;
        uploadStatus.textContent = `Processing... ${seconds}s`;
      }, 1000);
    }

    function stopUploadTimer() {
      if (uploadTimer) {
        clearInterval(uploadTimer);
        uploadTimer = null;
      }
    }

    async function handleUpload(file) {
      startUploadTimer();

      const formData = new FormData();
      formData.append('file', file);

      try {
        const res = await fetch('/upload-image', { method: 'POST', body: formData });
        const data = await res.json();
        stopUploadTimer();

        if (!res.ok) {
          uploadStatus.className = 'upload-status err';
          uploadStatus.textContent = `Upload failed: ${data.detail || res.statusText}`;
          return;
        }

        sessionId = data.session_id;
        totalWords = data.word_count || 0;

        uploadStatus.className = 'upload-status ok';
        uploadStatus.textContent = `${totalWords} word${totalWords !== 1 ? 's' : ''} loaded`;

        // Show step 2
        stepConnect.classList.remove('hidden');
        stepConnect.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
      } catch (err) {
        stopUploadTimer();
        uploadStatus.className = 'upload-status err';
        uploadStatus.textContent = `Upload failed: ${err.message}`;
      }
    }

    // ===================== Start Practice =====================
    startBtn.addEventListener('click', startSession);

    async function startSession() {
      startBtn.disabled = true;
      startBtn.textContent = 'Connecting...';

      try {
        // Create playback AudioContext IMMEDIATELY inside click handler
        // (before any await) so browser allows it as a user gesture.
        playbackCtx = new AudioContext();
        // Explicitly resume in case the browser still marks it suspended.
        await playbackCtx.resume();
        nextPlayTime = 0;
        console.log('[Audio] Playback context created, state:', playbackCtx.state, 'sampleRate:', playbackCtx.sampleRate);

        // Request mic access
        micStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          }
        });

        // Set up AudioContext for capture
        audioCtx = new AudioContext({ sampleRate: micStream.getAudioTracks()[0].getSettings().sampleRate || 44100 });

        // Load worklet
        const blob = new Blob([workletCode], { type: 'application/javascript' });
        const url = URL.createObjectURL(blob);
        await audioCtx.audioWorklet.addModule(url);
        URL.revokeObjectURL(url);

        workletNode = new AudioWorkletNode(audioCtx, 'capture-processor', {
          processorOptions: { sampleRate: audioCtx.sampleRate }
        });

        workletNode.port.onmessage = (e) => {
          if (e.data.type === 'audio' && ws && ws.readyState === WebSocket.OPEN) {
            const pcmBuf = new Uint8Array(e.data.pcm);
            const protoBuf = encodeAudioFrame(pcmBuf, 16000, 1);
            ws.send(protoBuf);
          }
        };

        const source = audioCtx.createMediaStreamSource(micStream);
        source.connect(workletNode);
        // Don't connect worklet to destination (no feedback)

        // Reset score
        scoreCorrect = 0;
        scoreIncorrect = 0;
        currentWord = '-';
        updateScoreUI();

        // Open WebSocket
        const protocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
        const wsUrl = `${protocol}//${location.host}/pipecat/ws/${sessionId}`;
        ws = new WebSocket(wsUrl);
        ws.binaryType = 'arraybuffer';

        ws.addEventListener('open', () => {
          // Show session view
          stepUpload.classList.add('hidden');
          stepConnect.classList.add('hidden');
          sessionView.classList.remove('hidden');
          statusLabel.textContent = 'Listening...';
          micIndicator.className = 'mic-indicator listening';
        });

        ws.addEventListener('message', handleWsMessage);

        ws.addEventListener('close', (e) => {
          statusLabel.textContent = 'Session ended';
          micIndicator.className = 'mic-indicator';
          cleanup();
        });

        ws.addEventListener('error', () => {
          statusLabel.textContent = 'Connection error';
          micIndicator.className = 'mic-indicator';
          cleanup();
        });

      } catch (err) {
        startBtn.disabled = false;
        startBtn.textContent = 'Start Practice';
        if (err.name === 'NotAllowedError') {
          alert('Microphone access is required for voice interaction. Please allow microphone access and try again.');
        } else {
          alert(`Failed to start session: ${err.message}`);
        }
      }
    }

    // ===================== Protobuf encoder =====================
    // Encode a varint
    function encodeVarint(value) {
      const bytes = [];
      while (value > 0x7f) {
        bytes.push((value & 0x7f) | 0x80);
        value >>>= 7;
      }
      bytes.push(value & 0x7f);
      return new Uint8Array(bytes);
    }

    // Encode a protobuf field (tag + value)
    function encodeField(fieldNum, wireType, data) {
      const tag = encodeVarint((fieldNum << 3) | wireType);
      if (wireType === 0) {
        // varint value
        const val = encodeVarint(data);
        const out = new Uint8Array(tag.length + val.length);
        out.set(tag); out.set(val, tag.length);
        return out;
      } else if (wireType === 2) {
        // length-delimited
        const len = encodeVarint(data.length);
        const out = new Uint8Array(tag.length + len.length + data.length);
        out.set(tag); out.set(len, tag.length); out.set(data, tag.length + len.length);
        return out;
      }
      return tag;
    }

    // Encode AudioRawFrame and wrap in Frame
    // Frame { audio(field 2): AudioRawFrame { audio(3): bytes, sample_rate(4): uint32, num_channels(5): uint32 } }
    function encodeAudioFrame(pcmBytes, sampleRate, numChannels) {
      // AudioRawFrame fields
      const f3 = encodeField(3, 2, pcmBytes);                // audio = bytes
      const f4 = encodeField(4, 0, sampleRate);              // sample_rate = varint
      const f5 = encodeField(5, 0, numChannels);             // num_channels = varint
      // Concatenate AudioRawFrame
      const audioMsg = new Uint8Array(f3.length + f4.length + f5.length);
      audioMsg.set(f3); audioMsg.set(f4, f3.length); audioMsg.set(f5, f3.length + f4.length);
      // Wrap in Frame { audio(field 2) = audioMsg }
      return encodeField(2, 2, audioMsg);
    }

    // ===================== Protobuf decoder =====================
    // Pipecat uses protobuf-serialized Frame messages over WebSocket.
    // Frame { oneof frame { TextFrame text=1; AudioRawFrame audio=2;
    //         TranscriptionFrame transcription=3; MessageFrame message=4; } }
    function decodeVarint(buf, pos) {
      let result = 0, shift = 0;
      while (pos < buf.length) {
        const b = buf[pos++];
        result |= (b & 0x7f) << shift;
        if ((b & 0x80) === 0) return [result, pos];
        shift += 7;
      }
      return [result, pos];
    }

    function decodeProtobufFields(buf) {
      const fields = {};
      let pos = 0;
      while (pos < buf.length) {
        const [tag, newPos] = decodeVarint(buf, pos);
        pos = newPos;
        const fieldNum = tag >>> 3;
        const wireType = tag & 0x7;
        if (wireType === 0) {
          // varint
          const [val, p] = decodeVarint(buf, pos);
          pos = p;
          fields[fieldNum] = val;
        } else if (wireType === 2) {
          // length-delimited (string, bytes, embedded message)
          const [len, p] = decodeVarint(buf, pos);
          pos = p;
          fields[fieldNum] = buf.slice(pos, pos + len);
          pos += len;
        } else {
          // skip unknown wire types
          break;
        }
      }
      return fields;
    }

    function decodeString(uint8arr) {
      return new TextDecoder().decode(uint8arr);
    }

    function parseProtobufFrame(arrayBuffer) {
      const buf = new Uint8Array(arrayBuffer);
      // Outer Frame: oneof { text=1, audio=2, transcription=3, message=4 }
      const outer = decodeProtobufFields(buf);

      if (outer[2]) {
        // AudioRawFrame: { id=1, name=2, audio=3(bytes), sample_rate=4, num_channels=5 }
        const audioFields = decodeProtobufFields(outer[2]);
        const pcmBytes = audioFields[3]; // raw PCM int16 LE
        const sampleRate = audioFields[4] || 16000;
        if (pcmBytes && pcmBytes.length > 0) {
          return { type: 'audio', pcm: pcmBytes, sampleRate };
        }
      }

      if (outer[1]) {
        // TextFrame: { id=1, name=2, text=3 }
        const textFields = decodeProtobufFields(outer[1]);
        const text = textFields[3] ? decodeString(textFields[3]) : '';
        return { type: 'text', text };
      }

      if (outer[3]) {
        // TranscriptionFrame: { id=1, name=2, text=3, user_id=4, timestamp=5 }
        const txFields = decodeProtobufFields(outer[3]);
        const text = txFields[3] ? decodeString(txFields[3]) : '';
        return { type: 'transcription', text };
      }

      if (outer[4]) {
        // MessageFrame: { data=1 }
        const msgFields = decodeProtobufFields(outer[4]);
        const data = msgFields[1] ? decodeString(msgFields[1]) : '';
        try {
          return { type: 'message', data: JSON.parse(data) };
        } catch (e) {
          return { type: 'message', data };
        }
      }

      return null;
    }

    // ===================== WebSocket message handling =====================
    let wsMessageCount = 0;
    let audioFrameCount = 0;

    function handleWsMessage(event) {
      wsMessageCount++;
      if (event.data instanceof ArrayBuffer) {
        const frame = parseProtobufFrame(event.data);
        if (!frame) {
          console.log('[WS] msg#', wsMessageCount, 'binary', event.data.byteLength, 'bytes — could not parse');
          return;
        }

        if (frame.type === 'audio') {
          audioFrameCount++;
          if (audioFrameCount <= 5) {
            console.log('[Audio] frame#', audioFrameCount, 'size:', frame.pcm.length, 'rate:', frame.sampleRate,
              'first4:', Array.from(frame.pcm.slice(0, 4)).map(b => b.toString(16).padStart(2, '0')).join(' '),
              'ctxState:', playbackCtx?.state);
          }
          playAudioChunk(frame.pcm, frame.sampleRate);
        } else if (frame.type === 'transcription') {
          // ASR transcription from user speech — finalize any open bot entry first
          if (currentBotEntry) {
            currentBotEntry.querySelector('.transcript-text').classList.remove('interim');
            parseScoreFromText(botUtteranceBuffer);
            botUtteranceBuffer = '';
            currentBotEntry = null;
            isBotSpeaking = false;
            if (!isMuted) {
              micIndicator.className = 'mic-indicator listening';
              statusLabel.textContent = 'Listening...';
            }
          }
          if (!currentUserEntry) {
            currentUserEntry = addTranscriptEntry('user');
          }
          updateTranscriptText(currentUserEntry, frame.text, false);
          currentUserEntry = null;
        } else if (frame.type === 'text') {
          // Raw LLM tokens — ignore for display; tts_update handles transcript
          // synchronized with audio playback via TranscriptBridge.
        } else if (frame.type === 'message') {
          // JSON messages (tts_update, tts_end, asr_update, asr_end, etc.)
          handleTranscriptMessage(frame.data);
        }
      } else {
        // Text WebSocket message (shouldn't happen with protobuf, but handle gracefully)
        try {
          const msg = JSON.parse(event.data);
          handleTranscriptMessage(msg);
        } catch (e) {
          // Ignore
        }
      }
    }

    // Buffer to accumulate full bot utterance for score parsing
    let botUtteranceBuffer = '';
    let botUtteranceTimer = null;

    // Parse bot utterance after 1.5s of no new text (bot finished speaking)
    function resetBotUtteranceTimer() {
      if (botUtteranceTimer) clearTimeout(botUtteranceTimer);
      botUtteranceTimer = setTimeout(() => {
        if (botUtteranceBuffer) {
          parseScoreFromText(botUtteranceBuffer);
          if (currentBotEntry) {
            currentBotEntry.querySelector('.transcript-text').classList.remove('interim');
            currentBotEntry = null;
          }
          botUtteranceBuffer = '';
          isBotSpeaking = false;
          if (!isMuted) {
            micIndicator.className = 'mic-indicator listening';
            statusLabel.textContent = 'Listening...';
          }
        }
      }, 1500);
    }

    function handleTranscriptMessage(msg) {
      const type = msg.type;

      if (type === 'asr_update') {
        // Interim user speech
        if (!currentUserEntry) {
          currentUserEntry = addTranscriptEntry('user');
        }
        updateTranscriptText(currentUserEntry, msg.text || msg.data || '', true);
      } else if (type === 'asr_end') {
        // Final user speech
        if (!currentUserEntry) {
          currentUserEntry = addTranscriptEntry('user');
        }
        updateTranscriptText(currentUserEntry, msg.text || msg.data || '', false);
        currentUserEntry = null;
      } else if (type === 'tts_update') {
        // Bot speaking text
        isBotSpeaking = true;
        micIndicator.className = 'mic-indicator bot-speaking';
        statusLabel.textContent = 'Tutor is speaking...';

        if (!currentBotEntry) {
          currentBotEntry = addTranscriptEntry('bot');
          botUtteranceBuffer = '';
        }
        // BotTranscriptSynchronization sends accumulated text, not deltas — replace buffer
        const newText = msg.text || msg.data || '';
        botUtteranceBuffer = newText;
        updateTranscriptText(currentBotEntry, botUtteranceBuffer, true);
      } else if (type === 'tts_end') {
        // Bot done speaking — parse score from full utterance
        if (currentBotEntry) {
          const textEl = currentBotEntry.querySelector('.transcript-text');
          textEl.classList.remove('interim');
          currentBotEntry = null;
        }
        parseScoreFromText(botUtteranceBuffer);
        botUtteranceBuffer = '';

        isBotSpeaking = false;
        if (!isMuted) {
          micIndicator.className = 'mic-indicator listening';
          statusLabel.textContent = 'Listening...';
        }
      }
    }

    // ===================== Transcript UI =====================
    function addTranscriptEntry(role) {
      // Remove empty placeholder
      const empty = transcript.querySelector('.transcript-empty');
      if (empty) empty.remove();

      const entry = document.createElement('div');
      entry.className = `transcript-entry ${role}`;

      const label = document.createElement('div');
      label.className = `transcript-label ${role}`;
      label.textContent = role === 'user' ? 'You' : 'Tutor';

      const text = document.createElement('div');
      text.className = 'transcript-text';

      entry.appendChild(label);
      entry.appendChild(text);
      transcript.appendChild(entry);
      transcript.scrollTop = transcript.scrollHeight;

      return entry;
    }

    function updateTranscriptText(entry, text, interim) {
      const textEl = entry.querySelector('.transcript-text');
      textEl.textContent = text;
      if (interim) {
        textEl.classList.add('interim');
      } else {
        textEl.classList.remove('interim');
      }
      transcript.scrollTop = transcript.scrollHeight;
    }

    // ===================== Audio Playback =====================
    // Find the "data" sub-chunk in a WAV file and return the PCM byte offset + length
    function findWavDataChunk(buf) {
      // Minimum WAV: RIFF(4) + size(4) + WAVE(4) + fmt (8+16) + data(8) = 44
      if (buf.length < 44) return null;
      if (buf[0] !== 0x52 || buf[1] !== 0x49 || buf[2] !== 0x46 || buf[3] !== 0x46) return null; // "RIFF"
      // Walk sub-chunks after "WAVE" at offset 12
      let pos = 12;
      while (pos + 8 <= buf.length) {
        const id = String.fromCharCode(buf[pos], buf[pos+1], buf[pos+2], buf[pos+3]);
        const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
        const chunkSize = view.getUint32(pos + 4, true);
        if (id === 'data') {
          return { offset: pos + 8, length: chunkSize };
        }
        pos += 8 + chunkSize;
        // Chunks are word-aligned
        if (chunkSize % 2 !== 0) pos++;
      }
      return null;
    }

    function playAudioChunk(audioBytes, sampleRate) {
      if (!playbackCtx) {
        console.warn('[Audio] No playback context, dropping chunk');
        return;
      }

      // Ensure context is running
      if (playbackCtx.state === 'suspended') {
        playbackCtx.resume().then(() => console.log('[Audio] Playback context resumed'));
      }

      // Reset play time if we haven't received audio in a while (gap between utterances)
      const now = playbackCtx.currentTime;
      const diff = now - lastMessageTime;
      if (nextPlayTime === 0 || diff > PLAY_TIME_RESET_THRESHOLD) {
        nextPlayTime = now + 0.05; // small buffer to avoid scheduling in the past
      }
      // If nextPlayTime fell behind current time, catch up
      if (nextPlayTime < now) {
        nextPlayTime = now + 0.02;
      }
      lastMessageTime = now;

      // Extract PCM data — strip WAV header if present
      let pcmData = audioBytes;
      const rate = sampleRate || 16000;

      const wavChunk = findWavDataChunk(audioBytes);
      if (wavChunk) {
        pcmData = audioBytes.slice(wavChunk.offset, wavChunk.offset + wavChunk.length);
        if (audioFrameCount <= 3) {
          console.log('[Audio] WAV header found, PCM offset:', wavChunk.offset, 'length:', wavChunk.length);
        }
      } else if (audioFrameCount <= 3) {
        console.log('[Audio] No WAV header, treating as raw PCM, length:', audioBytes.length);
      }

      const numSamples = Math.floor(pcmData.length / 2);
      if (numSamples <= 0) return;

      // Convert int16 LE PCM to float32
      const audioBuffer = playbackCtx.createBuffer(1, numSamples, rate);
      const channelData = audioBuffer.getChannelData(0);
      const view = new DataView(pcmData.buffer, pcmData.byteOffset, pcmData.byteLength);
      for (let i = 0; i < numSamples; i++) {
        channelData[i] = view.getInt16(i * 2, true) / 32768.0;
      }

      const source = playbackCtx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(playbackCtx.destination);
      source.start(nextPlayTime);
      nextPlayTime += audioBuffer.duration;
      activeSources.push(source);
      source.onended = () => {
        const idx = activeSources.indexOf(source);
        if (idx >= 0) activeSources.splice(idx, 1);
      };

      if (audioFrameCount <= 5) {
        console.log('[Audio] Scheduled', numSamples, 'samples at', rate, 'Hz, duration:', audioBuffer.duration.toFixed(3), 's, playAt:', nextPlayTime.toFixed(3));
      }
    }

    // ===================== Mute / End =====================
    muteBtn.addEventListener('click', () => {
      isMuted = !isMuted;
      muteBtn.textContent = isMuted ? 'Unmute' : 'Mute';

      if (workletNode) {
        workletNode.port.postMessage({ type: 'mute', value: isMuted });
      }

      if (isMuted) {
        micIndicator.className = 'mic-indicator muted';
        statusLabel.textContent = 'Muted';
      } else if (isBotSpeaking) {
        micIndicator.className = 'mic-indicator bot-speaking';
        statusLabel.textContent = 'Tutor is speaking...';
      } else {
        micIndicator.className = 'mic-indicator listening';
        statusLabel.textContent = 'Listening...';
      }
    });

    endBtn.addEventListener('click', () => {
      const attempted = scoreCorrect + scoreIncorrect;
      if (attempted < 25) {
        const remaining = 25 - attempted;
        if (!confirm(`You've only practiced ${attempted} word${attempted !== 1 ? 's' : ''} so far! Try to do at least 25. Only ${remaining} more to go!\n\nAre you sure you want to stop?`)) {
          return;
        }
      }
      if (ws) ws.close();
      cleanup();
      // Return to upload view
      sessionView.classList.add('hidden');
      stepUpload.classList.remove('hidden');
      stepConnect.classList.remove('hidden');
      // Reset transcript and score
      transcript.innerHTML = '<div class="transcript-empty">Waiting for conversation to start...</div>';
      currentUserEntry = null;
      currentBotEntry = null;
      botUtteranceBuffer = '';
      statusLabel.textContent = 'Connecting...';
    });

    function cleanup() {
      if (botUtteranceTimer) { clearTimeout(botUtteranceTimer); botUtteranceTimer = null; }
      // Stop all active audio sources
      activeSources.forEach(s => { try { s.stop(); } catch(e) {} });
      activeSources = [];
      if (micStream) {
        micStream.getTracks().forEach(t => t.stop());
        micStream = null;
      }
      if (audioCtx) {
        audioCtx.close().catch(() => {});
        audioCtx = null;
      }
      if (playbackCtx) {
        playbackCtx.close().catch(() => {});
        playbackCtx = null;
      }
      workletNode = null;
      isMuted = false;
      isBotSpeaking = false;
      nextPlayTime = 0;
      lastMessageTime = 0;
      wsMessageCount = 0;
      audioFrameCount = 0;
      muteBtn.textContent = 'Mute';
      startBtn.disabled = false;
      startBtn.textContent = 'Start Practice';
    }
    </script>
  </body>
</html>
