<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Spelling Bee Assistant</title>
    <style>
      * { box-sizing: border-box; }

      body {
        font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
        margin: 24px;
        max-width: 900px;
        color: #222;
      }

      h1 { margin-bottom: 4px; }

      .hint { color: #666; }

      .card {
        border: 1px solid #ddd;
        border-radius: 12px;
        padding: 16px;
        margin: 12px 0;
      }

      .card.hidden { display: none; }

      .card h2 { margin-top: 0; }

      .row {
        display: flex;
        gap: 16px;
        flex-wrap: wrap;
      }

      /* --- Buttons --- */
      button {
        padding: 10px 14px;
        border-radius: 10px;
        border: 1px solid #bbb;
        background: #fff;
        cursor: pointer;
        font-size: 0.95rem;
      }

      button.primary {
        background: #111;
        color: #fff;
        border-color: #111;
      }

      button.danger {
        background: #c30;
        color: #fff;
        border-color: #c30;
      }

      button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      /* --- Upload area --- */
      .drop-zone {
        border: 2px dashed #ccc;
        border-radius: 12px;
        padding: 2rem;
        text-align: center;
        cursor: pointer;
        transition: border-color 0.2s, background 0.2s;
        position: relative;
      }

      .drop-zone:hover,
      .drop-zone.dragover {
        border-color: #111;
        background: #f5f5f5;
      }

      .drop-zone input[type="file"] {
        position: absolute;
        inset: 0;
        opacity: 0;
        cursor: pointer;
      }

      .drop-zone-icon {
        font-size: 2rem;
        margin-bottom: 0.5rem;
      }

      .drop-zone-text {
        color: #666;
        font-size: 0.9rem;
      }

      .upload-status {
        margin-top: 10px;
        font-size: 0.95rem;
        min-height: 1.4rem;
      }

      .upload-status.ok { color: #0a7; font-weight: 600; }
      .upload-status.err { color: #c30; font-weight: 600; }
      .upload-status.loading { color: #666; }

      /* --- Session status bar --- */
      .session-status {
        display: flex;
        align-items: center;
        gap: 12px;
        padding: 12px 16px;
        border: 1px solid #ddd;
        border-radius: 12px;
        margin-bottom: 12px;
      }

      .mic-indicator {
        width: 12px;
        height: 12px;
        border-radius: 50%;
        background: #aaa;
        transition: background 0.3s;
        flex-shrink: 0;
      }

      .mic-indicator.listening {
        background: #0a7;
        box-shadow: 0 0 6px #0a788;
        animation: pulse 1.5s infinite;
      }

      .mic-indicator.bot-speaking {
        background: #111;
        box-shadow: 0 0 6px #11188;
        animation: pulse 1s infinite;
      }

      .mic-indicator.muted {
        background: #c30;
      }

      @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
      }

      .status-label {
        font-size: 0.9rem;
        color: #666;
        flex: 1;
      }

      .session-controls {
        display: flex;
        gap: 8px;
      }

      /* --- Transcript --- */
      .transcript {
        border: 1px solid #ddd;
        border-radius: 12px;
        padding: 16px;
        overflow-y: auto;
        min-height: 300px;
        max-height: calc(100vh - 300px);
      }

      .transcript-entry {
        margin-bottom: 12px;
        animation: fadeIn 0.2s ease-in;
      }

      @keyframes fadeIn {
        from { opacity: 0; transform: translateY(4px); }
        to { opacity: 1; transform: translateY(0); }
      }

      .transcript-label {
        font-size: 0.75rem;
        font-weight: 700;
        text-transform: uppercase;
        letter-spacing: 0.03em;
        margin-bottom: 2px;
      }

      .transcript-label.user { color: #666; }
      .transcript-label.bot { color: #111; }

      .transcript-text {
        padding: 8px 12px;
        border-radius: 10px;
        font-size: 0.95rem;
        line-height: 1.4;
        display: inline-block;
        max-width: 90%;
      }

      .transcript-entry.user .transcript-text {
        background: #f1f3f4;
      }

      .transcript-entry.bot .transcript-text {
        background: #f0f9f4;
      }

      .transcript-text.interim {
        opacity: 0.6;
        font-style: italic;
      }

      .transcript-empty {
        text-align: center;
        color: #666;
        padding: 3rem 1rem;
        font-size: 0.95rem;
      }
    </style>
  </head>
  <body>
    <h1>Spelling Bee Assistant</h1>
    <p class="hint">Practice spelling with a voice-powered AI tutor</p>

      <!-- Step 1: Upload -->
      <div id="stepUpload" class="card">
        <h2>1) Upload word list image</h2>
        <div class="drop-zone" id="dropZone">
          <input type="file" id="imageFile" accept="image/*" />
          <div class="drop-zone-icon">&#128247;</div>
          <div class="drop-zone-text">
            Drop a word list image here or click to browse
          </div>
        </div>
        <div class="upload-status" id="uploadStatus"></div>
      </div>

      <!-- Step 2: Connect -->
      <div id="stepConnect" class="card hidden">
        <h2>2) Start session</h2>
        <p class="hint" style="margin-bottom: 12px;">
          This will request microphone access and connect to the spelling tutor.
        </p>
        <button class="primary" id="startBtn">Start Session</button>
      </div>

      <!-- Step 3: Session -->
      <div id="sessionView" class="card hidden">
        <h2>3) Voice session</h2>
        <div class="session-status">
          <div class="mic-indicator" id="micIndicator"></div>
          <span class="status-label" id="statusLabel">Connecting...</span>
          <div class="session-controls">
            <button id="muteBtn" title="Mute microphone">Mute</button>
            <button class="danger" id="endBtn">End Session</button>
          </div>
        </div>
        <div class="transcript" id="transcript">
          <div class="transcript-empty">Waiting for conversation to start...</div>
        </div>
      </div>

    <script>
    // ===================== Audio Worklet Processor (inline) =====================
    const workletCode = `
class CaptureProcessor extends AudioWorkletProcessor {
  constructor(options) {
    super();
    this.targetRate = 16000;
    this.nativeRate = options.processorOptions.sampleRate || sampleRate;
    this.ratio = this.nativeRate / this.targetRate;
    // 32ms at 16kHz = 512 samples
    this.chunkSize = 512;
    this.buffer = new Float32Array(0);
    this.resamplePos = 0;
    this.muted = false;
    this.port.onmessage = (e) => {
      if (e.data.type === 'mute') this.muted = e.data.value;
    };
  }

  process(inputs) {
    const input = inputs[0];
    if (!input || !input[0]) return true;
    const samples = input[0];

    // Resample to 16kHz using linear interpolation
    const outputLen = Math.floor(samples.length / this.ratio);
    const resampled = new Float32Array(outputLen);
    for (let i = 0; i < outputLen; i++) {
      const srcIdx = i * this.ratio;
      const idx0 = Math.floor(srcIdx);
      const idx1 = Math.min(idx0 + 1, samples.length - 1);
      const frac = srcIdx - idx0;
      resampled[i] = samples[idx0] * (1 - frac) + samples[idx1] * frac;
    }

    // Append to buffer
    const newBuf = new Float32Array(this.buffer.length + resampled.length);
    newBuf.set(this.buffer);
    newBuf.set(resampled, this.buffer.length);
    this.buffer = newBuf;

    // Emit 512-sample chunks
    while (this.buffer.length >= this.chunkSize) {
      const chunk = this.buffer.slice(0, this.chunkSize);
      this.buffer = this.buffer.slice(this.chunkSize);

      // Convert float32 to int16
      const pcm = new Int16Array(this.chunkSize);
      for (let i = 0; i < this.chunkSize; i++) {
        const s = this.muted ? 0 : Math.max(-1, Math.min(1, chunk[i]));
        pcm[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      this.port.postMessage({ type: 'audio', pcm: pcm.buffer }, [pcm.buffer]);
    }
    return true;
  }
}
registerProcessor('capture-processor', CaptureProcessor);
`;

    // ===================== State =====================
    let sessionId = null;
    let ws = null;
    let audioCtx = null;
    let micStream = null;
    let workletNode = null;
    let isMuted = false;
    let isBotSpeaking = false;

    // Playback queue
    let playbackCtx = null;
    let nextPlayTime = 0;

    // Transcript
    let currentUserEntry = null;
    let currentBotEntry = null;

    // ===================== DOM refs =====================
    const stepUpload = document.getElementById('stepUpload');
    const stepConnect = document.getElementById('stepConnect');
    const sessionView = document.getElementById('sessionView');
    const dropZone = document.getElementById('dropZone');
    const imageFile = document.getElementById('imageFile');
    const uploadStatus = document.getElementById('uploadStatus');
    const startBtn = document.getElementById('startBtn');
    const muteBtn = document.getElementById('muteBtn');
    const endBtn = document.getElementById('endBtn');
    const micIndicator = document.getElementById('micIndicator');
    const statusLabel = document.getElementById('statusLabel');
    const transcript = document.getElementById('transcript');

    // ===================== Upload =====================
    dropZone.addEventListener('dragover', (e) => {
      e.preventDefault();
      dropZone.classList.add('dragover');
    });

    dropZone.addEventListener('dragleave', () => {
      dropZone.classList.remove('dragover');
    });

    dropZone.addEventListener('drop', (e) => {
      e.preventDefault();
      dropZone.classList.remove('dragover');
      if (e.dataTransfer.files.length) {
        imageFile.files = e.dataTransfer.files;
        handleUpload(e.dataTransfer.files[0]);
      }
    });

    imageFile.addEventListener('change', () => {
      if (imageFile.files.length) {
        handleUpload(imageFile.files[0]);
      }
    });

    async function handleUpload(file) {
      uploadStatus.className = 'upload-status loading';
      uploadStatus.textContent = 'Uploading and processing...';

      const formData = new FormData();
      formData.append('file', file);

      try {
        const res = await fetch('/upload-image', { method: 'POST', body: formData });
        const data = await res.json();

        if (!res.ok) {
          uploadStatus.className = 'upload-status err';
          uploadStatus.textContent = `Upload failed: ${data.detail || res.statusText}`;
          return;
        }

        sessionId = data.session_id;
        const wordCount = data.words ? data.words.length : 0;

        uploadStatus.className = 'upload-status ok';
        uploadStatus.textContent = `${wordCount} word${wordCount !== 1 ? 's' : ''} loaded`;

        // Show step 2
        stepConnect.classList.remove('hidden');
        stepConnect.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
      } catch (err) {
        uploadStatus.className = 'upload-status err';
        uploadStatus.textContent = `Upload failed: ${err.message}`;
      }
    }

    // ===================== Start Session =====================
    startBtn.addEventListener('click', startSession);

    async function startSession() {
      startBtn.disabled = true;
      startBtn.textContent = 'Connecting...';

      try {
        // Request mic access
        micStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          }
        });

        // Set up AudioContext for capture
        audioCtx = new AudioContext({ sampleRate: micStream.getAudioTracks()[0].getSettings().sampleRate || 44100 });

        // Load worklet
        const blob = new Blob([workletCode], { type: 'application/javascript' });
        const url = URL.createObjectURL(blob);
        await audioCtx.audioWorklet.addModule(url);
        URL.revokeObjectURL(url);

        workletNode = new AudioWorkletNode(audioCtx, 'capture-processor', {
          processorOptions: { sampleRate: audioCtx.sampleRate }
        });

        workletNode.port.onmessage = (e) => {
          if (e.data.type === 'audio' && ws && ws.readyState === WebSocket.OPEN) {
            const pcmBuf = e.data.pcm;
            const wavBuf = createWavChunk(new Int16Array(pcmBuf));
            ws.send(wavBuf);
          }
        };

        const source = audioCtx.createMediaStreamSource(micStream);
        source.connect(workletNode);
        // Don't connect worklet to destination (no feedback)

        // Set up playback context
        playbackCtx = new AudioContext({ sampleRate: 16000 });
        nextPlayTime = 0;

        // Open WebSocket
        const protocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
        const wsUrl = `${protocol}//${location.host}/pipecat/ws?session_id=${sessionId}`;
        ws = new WebSocket(wsUrl);
        ws.binaryType = 'arraybuffer';

        ws.addEventListener('open', () => {
          // Show session view
          stepUpload.classList.add('hidden');
          stepConnect.classList.add('hidden');
          sessionView.classList.remove('hidden');
          statusLabel.textContent = 'Listening...';
          micIndicator.className = 'mic-indicator listening';
        });

        ws.addEventListener('message', handleWsMessage);

        ws.addEventListener('close', (e) => {
          statusLabel.textContent = 'Session ended';
          micIndicator.className = 'mic-indicator';
          cleanup();
        });

        ws.addEventListener('error', () => {
          statusLabel.textContent = 'Connection error';
          micIndicator.className = 'mic-indicator';
          cleanup();
        });

      } catch (err) {
        startBtn.disabled = false;
        startBtn.textContent = 'Start Session';
        if (err.name === 'NotAllowedError') {
          alert('Microphone access is required for voice interaction. Please allow microphone access and try again.');
        } else {
          alert(`Failed to start session: ${err.message}`);
        }
      }
    }

    // ===================== WAV helpers =====================
    function createWavChunk(pcmInt16) {
      const numSamples = pcmInt16.length;
      const dataSize = numSamples * 2;
      const buffer = new ArrayBuffer(44 + dataSize);
      const view = new DataView(buffer);

      // RIFF header
      writeString(view, 0, 'RIFF');
      view.setUint32(4, 36 + dataSize, true);
      writeString(view, 8, 'WAVE');

      // fmt chunk
      writeString(view, 12, 'fmt ');
      view.setUint32(16, 16, true);         // chunk size
      view.setUint16(20, 1, true);           // PCM format
      view.setUint16(22, 1, true);           // mono
      view.setUint32(24, 16000, true);       // sample rate
      view.setUint32(28, 32000, true);       // byte rate
      view.setUint16(32, 2, true);           // block align
      view.setUint16(34, 16, true);          // bits per sample

      // data chunk
      writeString(view, 36, 'data');
      view.setUint32(40, dataSize, true);

      // PCM data
      const pcmView = new Int16Array(buffer, 44);
      pcmView.set(pcmInt16);

      return buffer;
    }

    function writeString(view, offset, str) {
      for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
      }
    }

    // ===================== WebSocket message handling =====================
    function handleWsMessage(event) {
      if (event.data instanceof ArrayBuffer) {
        // Binary = audio from server
        playAudioChunk(event.data);
      } else {
        // Text = JSON transcript message
        try {
          const msg = JSON.parse(event.data);
          handleTranscriptMessage(msg);
        } catch (e) {
          // Ignore non-JSON text
        }
      }
    }

    function handleTranscriptMessage(msg) {
      const type = msg.type;

      if (type === 'asr_update') {
        // Interim user speech
        if (!currentUserEntry) {
          currentUserEntry = addTranscriptEntry('user');
        }
        updateTranscriptText(currentUserEntry, msg.text || msg.data || '', true);
      } else if (type === 'asr_end') {
        // Final user speech
        if (!currentUserEntry) {
          currentUserEntry = addTranscriptEntry('user');
        }
        updateTranscriptText(currentUserEntry, msg.text || msg.data || '', false);
        currentUserEntry = null;
      } else if (type === 'tts_update') {
        // Bot speaking text
        isBotSpeaking = true;
        micIndicator.className = 'mic-indicator bot-speaking';
        statusLabel.textContent = 'Bot is speaking...';

        if (!currentBotEntry) {
          currentBotEntry = addTranscriptEntry('bot');
        }
        // Accumulate TTS text
        const existingText = currentBotEntry.querySelector('.transcript-text').textContent;
        const newText = msg.text || msg.data || '';
        updateTranscriptText(currentBotEntry, existingText + newText, true);
      } else if (type === 'tts_end') {
        // Bot done speaking
        if (currentBotEntry) {
          const textEl = currentBotEntry.querySelector('.transcript-text');
          textEl.classList.remove('interim');
          currentBotEntry = null;
        }
        isBotSpeaking = false;
        if (!isMuted) {
          micIndicator.className = 'mic-indicator listening';
          statusLabel.textContent = 'Listening...';
        }
      }
    }

    // ===================== Transcript UI =====================
    function addTranscriptEntry(role) {
      // Remove empty placeholder
      const empty = transcript.querySelector('.transcript-empty');
      if (empty) empty.remove();

      const entry = document.createElement('div');
      entry.className = `transcript-entry ${role}`;

      const label = document.createElement('div');
      label.className = `transcript-label ${role}`;
      label.textContent = role === 'user' ? 'You' : 'Tutor';

      const text = document.createElement('div');
      text.className = 'transcript-text';

      entry.appendChild(label);
      entry.appendChild(text);
      transcript.appendChild(entry);
      transcript.scrollTop = transcript.scrollHeight;

      return entry;
    }

    function updateTranscriptText(entry, text, interim) {
      const textEl = entry.querySelector('.transcript-text');
      textEl.textContent = text;
      if (interim) {
        textEl.classList.add('interim');
      } else {
        textEl.classList.remove('interim');
      }
      transcript.scrollTop = transcript.scrollHeight;
    }

    // ===================== Audio Playback =====================
    function playAudioChunk(arrayBuffer) {
      if (!playbackCtx) return;

      // Parse WAV: extract PCM data after 44-byte header
      const dataView = new DataView(arrayBuffer);
      let pcmOffset = 44;
      let pcmLength = arrayBuffer.byteLength - 44;

      // Validate minimum WAV size
      if (arrayBuffer.byteLength < 46) return;

      // Try to find actual data chunk offset for robustness
      if (arrayBuffer.byteLength >= 44) {
        // Read from header: data size is at byte 40
        const declaredSize = dataView.getUint32(40, true);
        if (declaredSize > 0 && declaredSize <= arrayBuffer.byteLength - 44) {
          pcmLength = declaredSize;
        }
      }

      const numSamples = pcmLength / 2;
      if (numSamples <= 0) return;

      const audioBuffer = playbackCtx.createBuffer(1, numSamples, 16000);
      const channelData = audioBuffer.getChannelData(0);

      for (let i = 0; i < numSamples; i++) {
        const sample = dataView.getInt16(pcmOffset + i * 2, true);
        channelData[i] = sample / 32768;
      }

      const source = playbackCtx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(playbackCtx.destination);

      const now = playbackCtx.currentTime;
      if (nextPlayTime < now) {
        nextPlayTime = now;
      }
      source.start(nextPlayTime);
      nextPlayTime += audioBuffer.duration;
    }

    // ===================== Mute / End =====================
    muteBtn.addEventListener('click', () => {
      isMuted = !isMuted;
      muteBtn.textContent = isMuted ? 'Unmute' : 'Mute';

      if (workletNode) {
        workletNode.port.postMessage({ type: 'mute', value: isMuted });
      }

      if (isMuted) {
        micIndicator.className = 'mic-indicator muted';
        statusLabel.textContent = 'Muted';
      } else if (isBotSpeaking) {
        micIndicator.className = 'mic-indicator bot-speaking';
        statusLabel.textContent = 'Bot is speaking...';
      } else {
        micIndicator.className = 'mic-indicator listening';
        statusLabel.textContent = 'Listening...';
      }
    });

    endBtn.addEventListener('click', () => {
      if (ws) ws.close();
      cleanup();
      // Return to upload view
      sessionView.classList.add('hidden');
      stepUpload.classList.remove('hidden');
      stepConnect.classList.remove('hidden');
      // Reset transcript
      transcript.innerHTML = '<div class="transcript-empty">Waiting for conversation to start...</div>';
      currentUserEntry = null;
      currentBotEntry = null;
      statusLabel.textContent = 'Connecting...';
    });

    function cleanup() {
      if (micStream) {
        micStream.getTracks().forEach(t => t.stop());
        micStream = null;
      }
      if (audioCtx) {
        audioCtx.close().catch(() => {});
        audioCtx = null;
      }
      if (playbackCtx) {
        playbackCtx.close().catch(() => {});
        playbackCtx = null;
      }
      workletNode = null;
      isMuted = false;
      isBotSpeaking = false;
      muteBtn.textContent = 'Mute';
      startBtn.disabled = false;
      startBtn.textContent = 'Start Session';
    }
    </script>
  </body>
</html>
